{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEGWH5Ib90wlZVRdc+n7y5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomPhanAnh/Kaggle-Solution/blob/main/Full_model_RIO_Technology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "-ydl72NrKhcz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import ast # return string\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Input dataset** \n",
        "\n",
        "Lấy dữ liệu từ DYNO"
      ],
      "metadata": {
        "id": "13Op9LigK97-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dữ liệu DYNO**"
      ],
      "metadata": {
        "id": "80RleKV_na-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "\n",
        "class DYNO_get_data:\n",
        "    \n",
        "    '''Class to get data'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        print('Change directory to the folder containing data')\n",
        "        print('Change name of columns in dataframe (phone and reg_citizen) for merge')\n",
        "        print('If customers register ids, remember to use reg_citizen instead of citizen - which use for id from phone')\n",
        "        \n",
        "        self.token = \"599450a72c558c1f60cf4f6c8fe5247683524c38a1afe3d348383830\"\n",
        "\n",
        "        self.API_links = ['https://gateway.dyno.me/dase/mapp/facebook/phone/',\n",
        "                         'https://gateway.dyno.me/dase/mapp/citizen/phone/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/is_valid/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/name/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/relationship/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/location/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/location/phone/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/location/citizen/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/education/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/education/phone/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/age/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/age/phone/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/work/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/work/phone/',\n",
        "                         #'https://gateway.dyno.me/dase/demo/face/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/conn/friend_count/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/conn/top_friends/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/adva/arpu/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/adva/arpu/phone/',\n",
        "                         #'https://gateway.dyno.me/dase/adva/pay_first/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/adva/pay_first/phone/',\n",
        "                         #'https://gateway.dyno.me/dase/adva/credit_score/facebook/',\n",
        "                         'https://gateway.dyno.me/dase/adva/credit_score/phone/',\n",
        "                         #'https://gateway.dyno.me/dase/adva/post/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/adva/post/phone/',\n",
        "                         #'https://gateway.dyno.me/dase/adva/img_tagged/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/adva/img_tagged/phone/',\n",
        "                         #'https://gateway.dyno.me/dase/adva/img/facebook/',\n",
        "                         #'https://gateway.dyno.me/dase/adva/img/phone/',\n",
        "                         'https://gateway.dyno.me/dase/adva/ta/citizen/',\n",
        "                         'https://gateway.dyno.me/dase/adva/sn/citizen/',\n",
        "                         'https://gateway.dyno.me/dase/adva/fi/citizen/',\n",
        "                         'https://gateway.dyno.me/dase/adva/dsi/citizen/'] \n",
        "        \n",
        "        # Create dictionary of mapp to link\n",
        "        self.dict_mapp_link = self.create_dict_mapp_link()\n",
        "        \n",
        "    def create_dict_mapp_link(self):\n",
        "        dict_mapp_link = {}\n",
        "        for link in self.API_links:\n",
        "            list_words = link.split('/')\n",
        "            mapp_name = list_words[-2] + '_to_' + list_words[-3]\n",
        "            dict_mapp_link[mapp_name] = link\n",
        "        \n",
        "        # To map registered ids\n",
        "        dict_mapp_link['reg_citizen_to_ta'] = dict_mapp_link['citizen_to_ta']\n",
        "        dict_mapp_link['reg_citizen_to_sn'] = dict_mapp_link['citizen_to_sn']\n",
        "        dict_mapp_link['reg_citizen_to_fi'] = dict_mapp_link['citizen_to_fi']\n",
        "        dict_mapp_link['reg_citizen_to_dsi'] = dict_mapp_link['citizen_to_dsi']\n",
        "        \n",
        "        return dict_mapp_link\n",
        "    \n",
        "    def mapp_info(self, mapp, info, pbar):\n",
        "\n",
        "        ''' Return a dict of all the queried information from kind of mapp and input info'''\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.update(1)\n",
        "\n",
        "        # Get link API to request\n",
        "        try:\n",
        "            url = self.dict_mapp_link[mapp]\n",
        "            r = requests.get(url + str(info), headers={'Authorization': self.token})\n",
        "            content = json.loads(r.content)\n",
        "\n",
        "            if not content['has_error']:\n",
        "                return {info: content['data']}\n",
        "\n",
        "            # Quota exceeded\n",
        "            if content['error_message'] not in ['PROFILE NOT FOUND', 'PROFILE NOT CRAWLED']:\n",
        "                print(content)\n",
        "\n",
        "            return {info: np.nan}\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Error at', info, 'with error:', e, '!!!')    \n",
        "            \n",
        "            return {}\n",
        "    \n",
        "    def get_data(self, list_info, mapp, max_workers=5):\n",
        "    \n",
        "        '''Get data for one type of mapp'''\n",
        "        \n",
        "        filename = mapp + '.pkl'\n",
        "        threads = []\n",
        "        mapp_file = {}\n",
        "\n",
        "        # Continue to append data\n",
        "        if os.path.exists(filename):\n",
        "            print('File exists')\n",
        "            mapp_file = pd.read_pickle(filename)\n",
        "\n",
        "        print(\"Getting data with\", max_workers, \"workers for\", filename, \"!\")\n",
        "\n",
        "        with tqdm(total=len(list_info)) as pbar:\n",
        "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "                for info in list_info:\n",
        "                    threads.append(executor.submit(self.mapp_info, mapp, info, pbar))\n",
        "                for task in as_completed(threads):\n",
        "                    mapp_file.update(task.result())\n",
        "\n",
        "                    # Save file every 100 records\n",
        "                    if len(mapp_file) % 100 == 0:\n",
        "                        pickle.dump(mapp_file, open(filename, \"wb\"))\n",
        "                        \n",
        "        # Save file\n",
        "        pickle.dump(mapp_file, open(filename, \"wb\"))\n",
        "    \n",
        "    def extract_dict_phone2id(self, filename='phone_to_citizen.pkl'):\n",
        "\n",
        "        ''' Extract CMNDs from citizen mapping from phone '''\n",
        "\n",
        "        phone_to_citizen = pd.read_pickle(filename)\n",
        "        dict_phone2id = {}\n",
        "\n",
        "        for k, v in phone_to_citizen.items():\n",
        "            if pd.notnull(v) and 'id' in v['ids'][0]:\n",
        "                dict_phone2id[k] = v['ids'][0]['id']\n",
        "\n",
        "        return dict_phone2id\n",
        "\n",
        "    def merge(self, df, filenames, use_id_from_phone=False):\n",
        "\n",
        "        '''Merge all dicts to original dataframe'''\n",
        "\n",
        "        if use_id_from_phone:\n",
        "            dict_phone2id = self.extract_dict_phone2id()\n",
        "            df['citizen'] = df['phone'].apply(lambda x: dict_phone2id[x] if x in dict_phone2id else np.nan)\n",
        "\n",
        "        for filename in filenames:\n",
        "            dict_mapping = pd.read_pickle(filename)\n",
        "            col_name = filename.split('.')[0]\n",
        "            info_name = filename.split('_to_')[0]\n",
        "            df[col_name] = df[info_name].apply(lambda x: dict_mapping[x] if x in dict_mapping else np.nan)\n",
        "\n",
        "        return df\n",
        "    \n",
        "    def check_error_info(self, list_info, file_name):\n",
        "        \n",
        "        '''Check error phones or ids while querying'''\n",
        "        \n",
        "        dict_mapping = pd.read_pickle(file_name)\n",
        "        \n",
        "        return [info for info in list_info if info not in dict_mapping]"
      ],
      "metadata": {
        "id": "J2T2BzIvLFUR"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1 Dữ liệu phân tích**"
      ],
      "metadata": {
        "id": "3TREp72nLHHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSaRQUK7K7-Q",
        "outputId": "c3bdeb3b-c941-4023-d45f-98708adcd428"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cd /content/drive/MyDrive/1. Colab Credit.vn/2. dataset/1. DYNO Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOn1-7lTNwZV",
        "outputId": "e81a97bc-f338-4595-cd7c-655079b8a424"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/1. Colab Credit.vn/2. dataset/1. DYNO Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read file csv\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/1. Colab Credit.vn/1. FI/2. Mcredit/selected_data.csv\")\n",
        "\n",
        "\n",
        "# Function to calculate age\n",
        "def to_datetime(x):\n",
        "  try:\n",
        "    return datetime.now().year - x\n",
        "  except:\n",
        "    return datetime.now().year - int(x[-4:])\n",
        "  return np.nan\n",
        "\n",
        "df['status'] = df['status'].astype(int)\n",
        "df = df[df['status'] != 0]\n",
        "df['status'] = df['status'].apply(lambda x: 0 if x == -1 else x)\n",
        "df['phone'] = df['phone'].apply(lambda x: '84' + str(x))\n",
        "\n",
        "def convert_income(x):\n",
        "    try:\n",
        "        x = float(x.replace('.', '').replace('$', ''))\n",
        "        if x <= 100:\n",
        "            return x * 1000000\n",
        "        if 100 < x < 100000:\n",
        "            return x * 1000\n",
        "    except:\n",
        "        return np.nan\n",
        "df['income'] = df['income'].apply(convert_income)\n",
        "\n",
        "df = df[['name', 'phone', 'reg_citizen', 'city', 'dob', 'income', 'status',\n",
        "       'citizen', 'phone_to_citizen', 'phone_to_credit_score', 'citizen_to_fi',\n",
        "       'citizen_to_sn', 'citizen_to_ta', 'citizen_to_dsi',\n",
        "       'reg_citizen_to_dsi', 'reg_citizen_to_fi', 'reg_citizen_to_phone',\n",
        "       'reg_citizen_to_sn', 'reg_citizen_to_ta']]\n",
        "province_mcredit = ['an giang',  'ba ria - vung tau', 'bac lieu', 'bac giang', 'bac ninh',\n",
        " 'ben tre', 'binh duong', 'binh dinh', 'binh phuoc', 'binh thuan', 'ca mau', 'can tho', 'da nang',\n",
        " 'dak lak', 'dong nai', 'dong thap', 'gia lai', 'ha nam', 'ha noi', 'ha tinh', 'hai duong', 'hai phong',\n",
        " 'hau giang', 'hung yen', 'khanh hoa', 'kien giang', 'lam dong', 'long an', 'nam dinh', 'nghe an',\n",
        " 'ninh binh', 'phu tho', 'quang ninh', 'soc trang', 'tay ninh', 'thai binh', 'thai nguyen', 'thanh hoa',\n",
        " 'thua thien hue', 'hue', 'tien giang', 'ho chi minh', 'tp hcm', 'vinh long', 'vinh phuc', 'yen bai']"
      ],
      "metadata": {
        "id": "9DMT5m0AK8Eg"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. Feature Engineering**"
      ],
      "metadata": {
        "id": "jxh6wba8LU47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove accent function\n",
        "s1 = u'ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ'\n",
        "s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'\n",
        "def remove_accents(input_str):\n",
        "  s = ''\n",
        "  for c in str(input_str):\n",
        "    if c in s1:\n",
        "      s += s0[s1.index(c)]\n",
        "    else:\n",
        "      s += c\n",
        "  return s\n",
        "\n",
        "# -------------------------------------------\n",
        "def transform_data(df, text, info, columns):\n",
        "  columns_first = ['name', 'phone', 'reg_citizen', 'city', 'dob', 'income', 'status']\n",
        "  try:\n",
        "    if text != \"phone_to_credit_score\":\n",
        "      return ast.literal_eval(df[text])[info][0][columns]\n",
        "    else:\n",
        "      \n",
        "      return ast.literal_eval(df[text])[columns]\n",
        "  except:\n",
        "    return np.nan\n",
        "# ------------------------------------------ Combine 2 data DYNO and customer provide\n",
        "cus_loan_id = [str(int(i)) for i in df[\"reg_citizen\"] if pd.notna(i)]\n",
        "columns = ['siCode', 'address', 'dob', 'gender', 'houseHoldCode', 'maBHYT', 'maSo',\n",
        "       'name', 'province', 'personalId']  \n",
        "def transform_fi(df,text,text2, info,cus_loan_id, columns_reg):\n",
        "  if pd.isnull(df[text]):\n",
        "    text, text2 = text2, text\n",
        "  try:\n",
        "    df1 = pd.DataFrame(ast.literal_eval(df[text])[info], columns = columns)\n",
        "    if not df[text2]:\n",
        "      df1 = pd.concat([df1, pd.DataFrame(ast.literal_eval(df[text2])[info], columns = columns)]) \n",
        "    df1.drop_duplicates(inplace=  True)\n",
        "    df1[\"province\"] = df1[\"province\"].apply(lambda x: x.replace(\"Tỉnh \",\"\").replace(\"Thành phố \",\"\"))\n",
        "    df1[\"province_score\"] = df1[\"province\"].apply(lambda x: 1 if remove_accents(x).lower() in province_mcredit else 0)\n",
        "    df1[\"fi_loan\"] = df1[\"personalId\"].apply(lambda x: 1 if x in cus_loan_id else 0) # Does people in that family also in loan\n",
        "    return df1.groupby(\"address\").agg({\"houseHoldCode\":\"nunique\", # number of household\n",
        "                                           \"maBHYT\":\"nunique\", # have BHYT\n",
        "                                           \"maSo\":\"nunique\", # have BHYT # flaon\n",
        "                                           \"province_score\":\"sum\", # count ablility to retrive money\n",
        "                                       \"personalId\":\"nunique\", # count number of people\n",
        "                                           \"fi_loan\":\"sum\"}).reset_index()[columns_reg][0] # number of loan human in family\n",
        "  except:\n",
        "    return np.nan  \n",
        "# ----------------------------------------\n",
        "def job_description(x):\n",
        "  if pd.isnull(x) or len(str(x)) == 0: return -1\n",
        "  if not x: x = x.lower()\n",
        "  level_2 = [\"phó hiệu trưởng\",\"bí thư\",\"đoàn\",\" giao dịch viên\", \"công an\", \"trưởng phòng\",\"phó phòng\", \"giám đốc\", \"quản lý\",\"tổ trưởng\", \"tổ phó\",\"hiệu trưởng\",\"cán bộ\",\"cb\",\"c.b\",\"c,b\",\"tổ\",\"cỏn bộ\",\"phó\",\"loeader\",\n",
        "             \"trưởng\",\"phó\",\"tp. marketing\",\"quản lý\",\"chủ tịch\",\"operator\",\"tp\",\"pp\"]\n",
        "  level_1 = [\"thao  tác viên\",\"bác sĩ\",\"đại diện bán hàng\",\"ks\",\"giảng viên\",\"văn thư\",\"phóng viên\",\n",
        "             \"td viên\",\"ltv\",\"bếp chính\",\"tín dụng\", \"gv\",\"thư ký\",\"văn phòng\",\"thủ kho\",\n",
        "             \"lập trình viên\",\"nghiên cứu viên\",\"phiên dịch\",\"kỹ thuật\",\"kỹ sư\",\"kĩ sư\",\n",
        "             \"trợ lý\",\"kiểm toán\",\"kế toán\",\"ksts\",\"giao dịch viên\", \"ktv\",\"?tv\",\"y sĩ\",\"y sỹ\",\n",
        "             \"thông dịch\",\"giáo viên\",\"trình dược viên\",\"văn hóa - thể thao\",\"vp\",\"tiếp thị\",\n",
        "             \"giám sát\",\"máy móc\",\"vh\",\"vận hành\"\n",
        "             \"quản lí chất lượng\",\"tiếp viên\",'tiếp tân',\"trợ thủ\",\"nh©n viªn \", \"bác sĩ\",\"giám sát\",\"thiết bị\",\n",
        "             \"chất lượng\",\"dược tá\",\"kiểm hóa\",\"y tế\",\"nh©n viªn\",\"trợ thủ\",\"nhân viên\",\"nv\",\"n,v\",\"thợ\",\"bảo trì\",\"viên\"]\n",
        "  if any([i for i in level_2 if i in x.lower()]):\n",
        "    return 2\n",
        "  elif any([i for i in level_1 if i in x.lower()]):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "# ---------------------------------------------\n",
        "def transform_another(df,text,text2, info,columns_ano, col):\n",
        "  if pd.isnull(df[text]):\n",
        "    text, text2 = text2, text\n",
        "  try:\n",
        "    if text not in [\"reg_citizen_to_dsi\", \"citizen_to_dsi\"]:\n",
        "      df1 = pd.DataFrame(ast.literal_eval(df[text])[info], columns = columns_ano, index = [0])\n",
        "      if not df[text2]:\n",
        "        df1 = pd.concat([df1, pd.DataFrame(ast.literal_eval(df[text2])[info], columns = columns_ano)]).drop_duplicates() \n",
        "    if not df[text2]:\n",
        "      df1 = pd.concat([df1, pd.DataFrame(ast.literal_eval(df[text2])[info], columns = columns_ano)]).drop_duplicates()\n",
        "    df1 = df1.sort_values(by = \"end_date\", ascending = False).loc[0].to_frame().T\n",
        "    return df1[col][0]\n",
        "  except:\n",
        "    return np.nan  \n",
        "# -------------------------------------------------\n",
        "def verify_city(x,y):\n",
        "  s1 = u'ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ'\n",
        "  s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'\n",
        " # ----------------------------- \n",
        "  try:\n",
        "    if not x or x.isnumeric(): \n",
        "      return 0\n",
        "    x = remove_accents(x).title()\n",
        "    y = remove_accents(y).title()\n",
        "    if any([x==y, x in y, y in x, x == y.rstrip(), x in y.rstrip(), x == y.rstrip()]):\n",
        "      return 1\n",
        "    return 0\n",
        "  except:\n",
        "    return 0\n",
        "# ------------------------------------------------------------------------------\n",
        "def verify_citizen(x,y):\n",
        "  try:\n",
        "    return 1 if int(x) == int(y) else 0\n",
        "  except:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "Kag1-OIYK8HD"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Dữ liệu đầu vào**"
      ],
      "metadata": {
        "id": "I8VrPLe7BaoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_p2c = ['name', 'id', 'phone', 'address', 'ward', 'district', 'province', 'dob', 'gender']\n",
        "columns_p2cc = ['action_avg', 'comment_avg', 'diff_day', 'education_level', 'freq_post','friend_education_level_0', 'friend_education_level_1', 'friend_education_level_2','friend_education_level_3', \n",
        "                'friend_pay_first', 'friend_pay_later', 'likes_count','mode_time_frame', 'post_count', 'reaction_avg', 'credit_score']\n",
        "columns_fi = [\"houseHoldCode\",\"maBHYT\",\"maSo\",\"province_score\",\"fi_loan\"]                \n",
        "columns_dsi = ['name', 'personal_id', 'gender', 'birthday', 'phone', 'permanent_residence','contact_address', 'start_date', 'end_date', 'position', 'salary', 'company_name', \n",
        "               'company_registration', 'company_address','company_tax_code', 'company_size_code', 'company_si_id', 'salary_percentile', 'si_contact_person_name', 'si_contact_person_phone',\n",
        "               'si_contact_person_email','health_insurance_id', 'social_insurance_id', 'maTG', 'phuongAn','wage_coefficient', 'total_salary_percentile']\n",
        "columns_sn = ['personalId', 'siCode', 'status', 'name', 'maBHYT', 'houseHoldCode', 'gender', 'dob', 'address', 'isHouseHolder']\n",
        "columns_dsi = [ 'permanent_residence','contact_address', 'start_date', 'end_date', 'position', 'salary','company_name', 'company_registration', 'company_address','company_tax_code',\n",
        "               'company_size_code', 'company_si_id', 'salary_percentile','si_contact_person_name', 'si_contact_person_phone', 'si_contact_person_email', 'health_insurance_id',\n",
        "               'social_insurance_id', 'maTG', 'phuongAn','wage_coefficient', 'total_salary_percentile']\n",
        "for i in columns_p2c:\n",
        "  df[i + \"p2c\"] = df.apply(lambda x: transform_data(x, \"phone_to_citizen\", \"ids\", i), axis = 1)\n",
        "for i in columns_p2cc:\n",
        "  df[i + \"p2cc\"] = df.apply(lambda x: transform_data(x,\"phone_to_credit_score\",\"ids\",i), axis = 1)\n",
        "for i in columns_fi:\n",
        "  df[i + \"fi\"] = df.apply(lambda x: transform_fi(x,\"citizen_to_fi\",\"reg_citizen_to_fi\",\"fi\",cus_loan_id, i), axis = 1) \n",
        "for i in columns_sn:\n",
        "  df[i + \"sn\"] = df.apply(lambda x: transform_another(x, \"citizen_to_sn\",\"reg_citizen_to_sn\",\"si\", columns_sn,i), axis = 1)\n",
        "for i in columns_dsi:\n",
        "  df[i + \"dsi\"] = df.apply(lambda x: transform_another(x, \"citizen_to_dsi\",\"reg_citizen_to_dsi\",\"detail_si\", columns_dsi, i), axis = 1)"
      ],
      "metadata": {
        "id": "jFncbp9tQ_50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Thông tin cá nhân**"
      ],
      "metadata": {
        "id": "uf140F0u_ZH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df[\"dob\"] == '0/80', \"dob\"] = 2022\n",
        "df.loc[df[\"dob\"] == '8', \"dob\"] = 2022\n",
        "df.loc[df[\"dob\"] == '/987', \"dob\"] = 1987\n",
        "df.loc[df[\"dob\"] ==  '993 ', \"dob\"] = 1993\n",
        "df.loc[df[\"dob\"] ==  '994 ', \"dob\"] = 1994\n",
        "df.loc[df[\"dob\"] ==  '0200', \"dob\"] = 2000\n",
        "df.loc[df[\"dob\"] ==  '979 ', \"dob\"] = 1979\n",
        "df.loc[df[\"dob\"] ==  '19i9', \"dob\"] = 1999\n",
        "df.loc[df[\"dob\"] ==  '9997', \"dob\"] = 1997\n",
        "df.loc[df[\"dob\"] ==  'g 11', \"dob\"] = 2022\n",
        "df.loc[df[\"dob\"] ==  '19i9', \"dob\"] = 1999\n",
        "df.loc[df[\"dob\"] ==  '1089-01-01', \"dob\"] = 1989\n",
        "\n",
        "df[\"full_info\"] = df.apply(lambda x: len(df.columns) - x.isnull().sum(), axis = 1) # Check full info\n",
        "df[\"ver_name\"] = df.apply(lambda x: verify_city(x[\"name\"],x[\"namep2c\"]), axis = 1) # Check correct name\n",
        "df[\"ver_id\"] = df.apply(lambda x: verify_citizen(x[\"idp2c\"], x[\"reg_citizen\"]), axis = 1) # Ver_id information\n",
        "df[\"dob\"] = df[\"dob\"].fillna(datetime.now())\n",
        "df[\"dobp2c\"] = df[\"dobp2c\"].fillna(datetime.now())\n",
        "df[\"dobp2c\"] = df[\"dobp2c\"].apply(lambda x: 2022 - int(pd.to_datetime(x).strftime(\"%Y\")))\n",
        "df[\"city_score\"] = df[\"city\"].apply(lambda x: 1 if x in province_mcredit else 0) # Calculate the city score\n",
        "df[\"dob\"] = df[\"dob\"].apply(lambda x: 2022 - int(x)) # Calculate the age\n",
        "df[\"ver_bod\"] = df.apply(lambda x: verify_citizen(x[\"dob\"],x[\"dobp2c\"]), axis = 1) # ver_birthday\n",
        "df = df.loc[(df[\"dob\"] > 18) & (df[\"dob\"] <= 100)] # Return last df"
      ],
      "metadata": {
        "id": "2EEc3jNKUMOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns = ['phone_to_citizen', 'phone_to_credit_score', 'citizen_to_fi',\n",
        "       'citizen_to_sn', 'citizen_to_ta', 'citizen_to_dsi',\n",
        "       'reg_citizen_to_dsi', 'reg_citizen_to_fi', 'reg_citizen_to_phone',\n",
        "       'reg_citizen_to_sn', 'reg_citizen_to_ta'], inplace = True)"
      ],
      "metadata": {
        "id": "B0hQUpAaDJvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 Thông tin công việc**"
      ],
      "metadata": {
        "id": "Br2oyKldAJNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ver_BHYT\"]    = df[\"health_insurance_iddsi\"].apply(lambda x: 0 if len(str(x)) == 0 and x.isnumeric() else 1)\n",
        "df[\"ver_BHXH\"]    = df[\"social_insurance_iddsi\"].apply(lambda x: 0 if len(str(x)) == 0 and x.isnumeric() else 1)\n",
        "df[\"position_score\"] = df[\"positiondsi\"].apply(lambda x: job_description(x))\n",
        "df[\"contact_addressdsi\"] = df[\"contact_addressdsi\"].apply(lambda x: x.split(\",\")[-1] if pd.notna(x) else x)\n",
        "df[\"contact_addressdsi\"] = df[\"contact_addressdsi\"].apply(lambda x: x.replace(\" Tỉnh \",\"\").replace(\" Thành phố \",\"\").lower() if pd.notna(x) else x)\n",
        "df[\"contact_addressdsi\"] = df[\"contact_addressdsi\"].apply(lambda x: 1 if remove_accents(x) in province_mcredit else 0 if pd.notna(x) else x)\n",
        "df[\"permanent_residencedsi\"] = df[\"permanent_residencedsi\"].apply(lambda x: x.split(\",\")[-1] if pd.notna(x) else x )\n",
        "df[\"permanent_residencedsi\"] = df[\"permanent_residencedsi\"].apply(lambda x: x.replace(\" Tỉnh \",\"\").replace(\" Thành phố \",\"\") if pd.notna(x) else x)\n",
        "df[\"permanent_residencedsi\"] = df[\"permanent_residencedsi\"].apply(lambda x: 1 if remove_accents(x) in province_mcredit else 0 if pd.notna(x) else x)\n",
        "df[\"company_size_code\"] = df.apply(lambda x: x[\"company_size_codedsi\"] if all([len(str(x[\"company_tax_codedsi\"])) > 5, len(x[\"company_addressdsi\"]) > 5]) else 0, axis = 1)\n",
        "df[\"company_size_code\"] = df[\"company_size_codedsi\"].map({0:0,\"A\":1,\"B\":2,'C':3,'D':4,'E':5,'F':6,'G':7,\"H\":8,'I':9})\n",
        "df.loc[df[\"salarydsi\"] == \"\", \"salarydsi\"] = np.nan\n",
        "df[\"salarydsi\"] = pd.to_numeric(df['salarydsi'], errors='coerce')\n",
        "df[\"contact_person\"] = df[\"si_contact_person_namedsi\"].apply(lambda x: 0 if len(str(x)) == 0 else 1)\n",
        "df[\"contact_phone\"] = df[\"si_contact_person_namedsi\"].apply(lambda x: 1 if len(str(x)) != 0 else 0)\n",
        "df[\"contact_email\"] = df[\"si_contact_person_namedsi\"].apply(lambda x: 0 if len(str(x)) == 0 and \"@\" in x else 1)\n",
        "df[\"contact\"] = (df[\"contact_email\"] + df[\"contact_phone\"] + df[\"contact_person\"]) /3  \n"
      ],
      "metadata": {
        "id": "k7FMaTg-0kMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Training model**"
      ],
      "metadata": {
        "id": "ZQdAybtvA1Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm==3.3.2"
      ],
      "metadata": {
        "id": "AGxfjLpKSeAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select_dtypes(['int','float']).columns"
      ],
      "metadata": {
        "id": "voUn8mnr6r_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "mtgcEMSZ9HKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"genderp2c\"] = df[\"genderp2c\"].map({\"M\":1, \"F\":0})\n",
        "df[\"gender\"] = pd.to_numeric(df.apply(lambda x: x[\"genderp2c\"] if pd.notna(x[\"genderp2c\"]) else x[\"gendersn\"], axis = 1))"
      ],
      "metadata": {
        "id": "nVjzSd3u9HOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "UWrqcqvg_qDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['houseHoldCodefi', 'maBHYTfi', 'maSofi', 'province_scorefi','fi_loanfi']]\n"
      ],
      "metadata": {
        "id": "R7I-XyOH_uAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgbm\n",
        "\n",
        "#     if 'credit_score' in col:\n",
        "#         features.append(col)\n",
        "\n",
        "X = df[['dob', 'ver_name', 'ver_id', 'ver_bod','ver_BHYT', 'ver_BHXH', 'city_score', 'gender', 'houseHoldCodefi', 'maBHYTfi', 'maSofi', 'province_scorefi','fi_loanfi',\"personalIdfi\"]]\n",
        "y = df['status']\n",
        "X.reset_index(drop=True, inplace=True)\n",
        "y.reset_index(drop=True, inplace=True)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "params = {\n",
        "      \"objective\": \"binary\", \n",
        "      \"metric\": \"auc\"\n",
        "  }\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "models = []\n",
        "feature_importance = 0\n",
        "score = 0 \n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
        "    print(\"Fold :\", fold + 1)\n",
        "    X_train, y_train = X.loc[train_idx], y[train_idx]  # Create dataset\n",
        "    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n",
        "    lgbm_train = lgbm.Dataset(X_train, y_train) #RMSPE weight\n",
        "    lgbm_valid = lgbm.Dataset(X_valid, y_valid, reference = lgbm_train)\n",
        "    model = lgbm.train(params=params,   # model \n",
        "                      train_set=lgbm_train,\n",
        "                      valid_sets=lgbm_valid,\n",
        "                      num_boost_round=2000,\n",
        "                      verbose_eval=100,\n",
        "                      early_stopping_rounds=100)\n",
        "    y_pred = model.predict(X_valid)# validation \n",
        "    auc = roc_auc_score(y_valid, y_pred)\n",
        "    print(f'Performance of the　prediction: , AUC: {auc}')\n",
        "    score += auc / 5 #keep scores and models\n",
        "    models.append(model)\n",
        "    feature_importance += model.feature_importance()/5\n",
        "    print(\"*\" * 100)\n",
        "print(f'Final score: {score}')\n",
        "feature_importance_df = pd.DataFrame(data={'columns': X.columns, 'feature_importance': feature_importance}).sort_values(by='feature_importance', ascending=False)\n",
        "plt.figure(figsize=(5, 10))\n",
        "sns.barplot(data=feature_importance_df, x='feature_importance', y='columns')"
      ],
      "metadata": {
        "id": "2QY3o1J7BE8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Test New model**"
      ],
      "metadata": {
        "id": "KMptqyK8BKLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1 No Undersampling** "
      ],
      "metadata": {
        "id": "ZWqo9j7vBpIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def overview_model(model, x_train, y_train, x_test, y_test, name):\n",
        "  return pd.DataFrame({\"accuracy\":accuracy_score(y_test, model.predict(x_test)),\n",
        "                       \"recall\": recall_score(y_test, model.predict(x_test)),\n",
        "                       \"precision\":precision_score(y_test, model.predict(x_test)),\n",
        "                       \"f1_score\":f1_score(y_test, model.predict(x_test))}, index = [\"name\"])"
      ],
      "metadata": {
        "id": "3NaK-vuAB06g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "x1 = df_input[['dob', 'ver_name', 'ver_id', 'ver_bod',\n",
        "       'city_score', 'full_info', 'gender_x',\n",
        "       'contact', 'houseHoldCode', 'maBHYT', 'maSo', 'personalId',\n",
        "       'province_score', 'fi_loan']]\n",
        "y1  = df_input[\"status\"]\n",
        "\n",
        "x_train,  x_test,y_train, y_test = train_test_split(x1, y1,\n",
        "                                                    test_size = 0.2, \n",
        "                                                    random_state = 7, \n",
        "                                                    stratify = y1)\n",
        "\n",
        "bst = xgb.XGBClassifier(booster = \"gbtree\", eta = 0.3, gamma = 0,\n",
        "                    max_depth= 3,\n",
        "                    sampling_method = \"uniform\",\n",
        "                    reg_lambda = 1, \n",
        "                    alpha = 0)\n",
        "bst.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "V2Qnt8k-BE-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "def clean_dataset(df):\n",
        "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
        "    df.dropna(inplace=True)\n",
        "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
        "    return df[indices_to_keep].astype(np.float64).reset_index()\n",
        "\n",
        "x1 = clean_dataset(df_input[[\"status\",'permanent_residence', 'contact_address',\n",
        "       'ver_BHYT', 'ver_BHXH', 'position_score', 'total_salary_percentile',\n",
        "       'workplace_score', 'company_size_code', 'houseHoldCode', 'maBHYT',\n",
        "       'maSo', 'personalId', 'province_score', 'fi_loan', 'ver_name', 'ver_id',\n",
        "       'ver_bod']]).drop(columns = [\"index\",\"status\"])\n",
        "y1  = clean_dataset(df_input[[\"status\",'permanent_residence', 'contact_address',\n",
        "       'ver_BHYT', 'ver_BHXH', 'position_score', 'total_salary_percentile',\n",
        "       'workplace_score', 'company_size_code', 'houseHoldCode', 'maBHYT',\n",
        "       'maSo', 'personalId', 'province_score', 'fi_loan', 'ver_name', 'ver_id',\n",
        "       'ver_bod']])[\"status\"]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x1, y1,\n",
        "                                                    test_size = 0.2, \n",
        "                                                    random_state = 7, \n",
        "                                                    stratify = y1)\n",
        "\n",
        "knn.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "qZvlZCejBFBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "P4jbihXaBFEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OMb98a-sBFHH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}